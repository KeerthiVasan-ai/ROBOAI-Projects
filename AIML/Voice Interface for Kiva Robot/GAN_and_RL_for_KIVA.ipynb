{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##GAN ARCH"
      ],
      "metadata": {
        "id": "hSvQ98TptuWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Creation"
      ],
      "metadata": {
        "id": "AauwfC1ttX89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaMEbM8glMeV",
        "outputId": "d309cf37-50d5-46a9-de1e-fb814d72d7c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "# Example sensor data\n",
        "# No data points are given\n",
        "# Assuming each data point has 10 sensor readings (features)\n",
        "\n",
        "sensor_data = np.random.rand(1000, 10)\n",
        "sensor_data_dim = sensor_data.shape[1]\n",
        "\n",
        "sensor_data_tensor = torch.tensor(sensor_data, dtype=torch.float32)\n",
        "\n",
        "# Create Batch for Training\n",
        "batch_size = 64\n",
        "dataloader = data.DataLoader(sensor_data_tensor, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "sensor_data_dim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "U_LnxrY4lyPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generator Definition"
      ],
      "metadata": {
        "id": "0mmWQFMktchM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "DsjxNy91mNBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Discriminator Definition"
      ],
      "metadata": {
        "id": "4PO0I2Ybtgq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "iBpP9BQamRpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining Hyperparameter"
      ],
      "metadata": {
        "id": "NR7P6ArOtlId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 100\n",
        "output_dim = sensor_data_dim\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "lr = 0.0002\n",
        "\n",
        "# Initialize the models\n",
        "generator = Generator(input_dim, output_dim)\n",
        "discriminator = Discriminator(output_dim)\n",
        "\n",
        "# Loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "bgi4k6xZmVV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "GJpGT2NdtqpU"
      }
    },
    {
      "source": [
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    for real_data in dataloader:\n",
        "        current_batch_size = real_data.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "\n",
        "        optimizer_d.zero_grad()\n",
        "        real_labels = torch.ones(current_batch_size, 1)\n",
        "        fake_labels = torch.zeros(current_batch_size, 1)\n",
        "\n",
        "        outputs = discriminator(real_data)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        z = torch.randn(current_batch_size, input_dim)\n",
        "        fake_data = generator(z)\n",
        "        outputs = discriminator(fake_data.detach())\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "\n",
        "        optimizer_g.zero_grad()\n",
        "        z = torch.randn(current_batch_size, input_dim)\n",
        "        fake_data = generator(z)\n",
        "        outputs = discriminator(fake_data)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] D Loss: {d_loss.item()} G Loss: {g_loss.item()}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wEqBxqomtYT",
        "outputId": "b7b2621b-6e24-4aea-ae5e-6f849e3e94bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] D Loss: 1.284868836402893 G Loss: 0.6848291158676147\n",
            "Epoch [2/100] D Loss: 1.3614052534103394 G Loss: 0.5779516100883484\n",
            "Epoch [3/100] D Loss: 1.4518139362335205 G Loss: 0.6057993769645691\n",
            "Epoch [4/100] D Loss: 1.3735291957855225 G Loss: 0.8725172281265259\n",
            "Epoch [5/100] D Loss: 1.2598413228988647 G Loss: 1.0636274814605713\n",
            "Epoch [6/100] D Loss: 1.2149579524993896 G Loss: 0.9045301675796509\n",
            "Epoch [7/100] D Loss: 1.4091651439666748 G Loss: 0.730186939239502\n",
            "Epoch [8/100] D Loss: 1.1350481510162354 G Loss: 0.9606666564941406\n",
            "Epoch [9/100] D Loss: 1.2005908489227295 G Loss: 0.6679550409317017\n",
            "Epoch [10/100] D Loss: 1.4977399110794067 G Loss: 0.49771609902381897\n",
            "Epoch [11/100] D Loss: 1.1668481826782227 G Loss: 0.8041625022888184\n",
            "Epoch [12/100] D Loss: 1.1140117645263672 G Loss: 0.7353340983390808\n",
            "Epoch [13/100] D Loss: 1.1719766855239868 G Loss: 0.7852832078933716\n",
            "Epoch [14/100] D Loss: 1.4245593547821045 G Loss: 0.5952092409133911\n",
            "Epoch [15/100] D Loss: 1.4148306846618652 G Loss: 0.648007333278656\n",
            "Epoch [16/100] D Loss: 0.9745533466339111 G Loss: 1.380454182624817\n",
            "Epoch [17/100] D Loss: 0.8651475310325623 G Loss: 1.456160306930542\n",
            "Epoch [18/100] D Loss: 1.077046513557434 G Loss: 0.9668585062026978\n",
            "Epoch [19/100] D Loss: 1.264064073562622 G Loss: 0.7990044355392456\n",
            "Epoch [20/100] D Loss: 1.233940839767456 G Loss: 0.9459388852119446\n",
            "Epoch [21/100] D Loss: 1.080751657485962 G Loss: 1.2299907207489014\n",
            "Epoch [22/100] D Loss: 1.0351256132125854 G Loss: 1.105812430381775\n",
            "Epoch [23/100] D Loss: 1.4174138307571411 G Loss: 0.6569098830223083\n",
            "Epoch [24/100] D Loss: 1.0643994808197021 G Loss: 1.5220741033554077\n",
            "Epoch [25/100] D Loss: 1.3558456897735596 G Loss: 0.7806954979896545\n",
            "Epoch [26/100] D Loss: 1.7060316801071167 G Loss: 0.5837677717208862\n",
            "Epoch [27/100] D Loss: 1.3608789443969727 G Loss: 1.137618899345398\n",
            "Epoch [28/100] D Loss: 1.1672052145004272 G Loss: 1.1867396831512451\n",
            "Epoch [29/100] D Loss: 1.3505768775939941 G Loss: 0.7658190727233887\n",
            "Epoch [30/100] D Loss: 1.4816398620605469 G Loss: 0.8262829780578613\n",
            "Epoch [31/100] D Loss: 1.366551160812378 G Loss: 1.0271698236465454\n",
            "Epoch [32/100] D Loss: 1.254206895828247 G Loss: 0.9506555795669556\n",
            "Epoch [33/100] D Loss: 1.557741641998291 G Loss: 0.6542311310768127\n",
            "Epoch [34/100] D Loss: 1.4904272556304932 G Loss: 0.6797804832458496\n",
            "Epoch [35/100] D Loss: 1.4786603450775146 G Loss: 0.7073867917060852\n",
            "Epoch [36/100] D Loss: 1.3621463775634766 G Loss: 0.9544910192489624\n",
            "Epoch [37/100] D Loss: 1.257690191268921 G Loss: 1.0399974584579468\n",
            "Epoch [38/100] D Loss: 1.3913015127182007 G Loss: 0.8951355218887329\n",
            "Epoch [39/100] D Loss: 1.5073364973068237 G Loss: 0.7980583310127258\n",
            "Epoch [40/100] D Loss: 1.5356345176696777 G Loss: 0.8110432624816895\n",
            "Epoch [41/100] D Loss: 1.4009196758270264 G Loss: 0.9665971994400024\n",
            "Epoch [42/100] D Loss: 1.2877388000488281 G Loss: 0.927580714225769\n",
            "Epoch [43/100] D Loss: 1.2756175994873047 G Loss: 0.8317245244979858\n",
            "Epoch [44/100] D Loss: 1.320655107498169 G Loss: 0.7524309754371643\n",
            "Epoch [45/100] D Loss: 1.3056862354278564 G Loss: 0.7848135232925415\n",
            "Epoch [46/100] D Loss: 1.376016616821289 G Loss: 0.7134894728660583\n",
            "Epoch [47/100] D Loss: 1.5248955488204956 G Loss: 0.5707403421401978\n",
            "Epoch [48/100] D Loss: 1.582313060760498 G Loss: 0.5249350666999817\n",
            "Epoch [49/100] D Loss: 1.556671380996704 G Loss: 0.5277588367462158\n",
            "Epoch [50/100] D Loss: 1.4894585609436035 G Loss: 0.5418187975883484\n",
            "Epoch [51/100] D Loss: 1.4716390371322632 G Loss: 0.5611623525619507\n",
            "Epoch [52/100] D Loss: 1.4587100744247437 G Loss: 0.5744607448577881\n",
            "Epoch [53/100] D Loss: 1.4671673774719238 G Loss: 0.5883536338806152\n",
            "Epoch [54/100] D Loss: 1.4332443475723267 G Loss: 0.6026350855827332\n",
            "Epoch [55/100] D Loss: 1.411828875541687 G Loss: 0.6174667477607727\n",
            "Epoch [56/100] D Loss: 1.3816008567810059 G Loss: 0.6322348117828369\n",
            "Epoch [57/100] D Loss: 1.3849711418151855 G Loss: 0.6420446038246155\n",
            "Epoch [58/100] D Loss: 1.3826780319213867 G Loss: 0.6387940645217896\n",
            "Epoch [59/100] D Loss: 1.3788273334503174 G Loss: 0.6390259861946106\n",
            "Epoch [60/100] D Loss: 1.3890984058380127 G Loss: 0.6515368223190308\n",
            "Epoch [61/100] D Loss: 1.399570107460022 G Loss: 0.6753508448600769\n",
            "Epoch [62/100] D Loss: 1.37419593334198 G Loss: 0.7028773427009583\n",
            "Epoch [63/100] D Loss: 1.344951868057251 G Loss: 0.7169389128684998\n",
            "Epoch [64/100] D Loss: 1.3596946001052856 G Loss: 0.7168139219284058\n",
            "Epoch [65/100] D Loss: 1.3746856451034546 G Loss: 0.7047351598739624\n",
            "Epoch [66/100] D Loss: 1.4216909408569336 G Loss: 0.6850727796554565\n",
            "Epoch [67/100] D Loss: 1.402634620666504 G Loss: 0.6861039400100708\n",
            "Epoch [68/100] D Loss: 1.3490808010101318 G Loss: 0.7141268849372864\n",
            "Epoch [69/100] D Loss: 1.2783913612365723 G Loss: 0.75536048412323\n",
            "Epoch [70/100] D Loss: 1.1824853420257568 G Loss: 0.7944117188453674\n",
            "Epoch [71/100] D Loss: 1.1554183959960938 G Loss: 0.783661425113678\n",
            "Epoch [72/100] D Loss: 1.2642936706542969 G Loss: 0.6954145431518555\n",
            "Epoch [73/100] D Loss: 1.4016940593719482 G Loss: 0.6383168697357178\n",
            "Epoch [74/100] D Loss: 1.4018150568008423 G Loss: 0.7443699836730957\n",
            "Epoch [75/100] D Loss: 1.3089005947113037 G Loss: 0.8394852876663208\n",
            "Epoch [76/100] D Loss: 1.2684794664382935 G Loss: 0.7677275538444519\n",
            "Epoch [77/100] D Loss: 1.3314526081085205 G Loss: 0.7531214952468872\n",
            "Epoch [78/100] D Loss: 1.2649192810058594 G Loss: 0.8466496467590332\n",
            "Epoch [79/100] D Loss: 1.308510184288025 G Loss: 0.8094651103019714\n",
            "Epoch [80/100] D Loss: 1.306992769241333 G Loss: 0.7087697982788086\n",
            "Epoch [81/100] D Loss: 1.2944872379302979 G Loss: 0.7315772771835327\n",
            "Epoch [82/100] D Loss: 1.0787837505340576 G Loss: 0.9605742692947388\n",
            "Epoch [83/100] D Loss: 1.007890224456787 G Loss: 0.906579315662384\n",
            "Epoch [84/100] D Loss: 1.429645299911499 G Loss: 0.5713151693344116\n",
            "Epoch [85/100] D Loss: 1.5137242078781128 G Loss: 0.6860263347625732\n",
            "Epoch [86/100] D Loss: 1.5117335319519043 G Loss: 0.7348814010620117\n",
            "Epoch [87/100] D Loss: 1.5631787776947021 G Loss: 0.6096208691596985\n",
            "Epoch [88/100] D Loss: 1.6366961002349854 G Loss: 0.5675182938575745\n",
            "Epoch [89/100] D Loss: 1.4574273824691772 G Loss: 0.6877230405807495\n",
            "Epoch [90/100] D Loss: 1.2356393337249756 G Loss: 0.8538116216659546\n",
            "Epoch [91/100] D Loss: 1.3186464309692383 G Loss: 0.7808845043182373\n",
            "Epoch [92/100] D Loss: 1.3083568811416626 G Loss: 0.7585922479629517\n",
            "Epoch [93/100] D Loss: 1.3153982162475586 G Loss: 0.8980199098587036\n",
            "Epoch [94/100] D Loss: 1.4633735418319702 G Loss: 0.8825531005859375\n",
            "Epoch [95/100] D Loss: 1.5812945365905762 G Loss: 0.8404690027236938\n",
            "Epoch [96/100] D Loss: 1.689904808998108 G Loss: 0.754257082939148\n",
            "Epoch [97/100] D Loss: 1.671950101852417 G Loss: 0.7547717094421387\n",
            "Epoch [98/100] D Loss: 1.4244942665100098 G Loss: 0.8303176164627075\n",
            "Epoch [99/100] D Loss: 1.2072598934173584 G Loss: 0.9388502240180969\n",
            "Epoch [100/100] D Loss: 1.1776916980743408 G Loss: 0.9228207468986511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##REINFORCEMENT LEARNING AGENT"
      ],
      "metadata": {
        "id": "4wj5WEietRvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RL AGENT DEFINITION"
      ],
      "metadata": {
        "id": "uLEbxrqIubU_"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros((state_dim, action_dim))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        state_array = np.array(state.detach())\n",
        "        state_index = state_array.astype(int)\n",
        "        return np.argmax(self.q_table[state_index])\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        next_state_array = np.array(next_state.detach()).flatten().astype(int)\n",
        "        best_next_action = np.argmax(self.q_table[next_state_array])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state_array, best_next_action]\n",
        "\n",
        "        state_array = np.array(state.detach()).flatten().astype(int)\n",
        "        td_error = td_target - self.q_table[state_array, action]\n",
        "        self.q_table[state_array, action] += self.lr * td_error"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "l08qcpUwpPDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_anomaly(data):\n",
        "    with torch.no_grad():\n",
        "        output = discriminator(data)\n",
        "    return output.mean().item() < 0.5"
      ],
      "metadata": {
        "id": "b_rF-BpdndLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TRAINING RL AGENT"
      ],
      "metadata": {
        "id": "fH4ig-MruYAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = sensor_data_dim\n",
        "action_dim = 2  # 0: normal, 1: anomaly\n",
        "\n",
        "# Initialize the Q-learning agent\n",
        "agent = QLearningAgent(state_dim, action_dim)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for real_data in dataloader:\n",
        "        state = real_data\n",
        "\n",
        "        # Agent chooses an action\n",
        "        action = agent.choose_action(state)\n",
        "\n",
        "        # Calculate reward based on action\n",
        "        if action == 1 and is_anomaly(real_data):\n",
        "            reward = 1\n",
        "        elif action == 0 and not is_anomaly(real_data):\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = -1\n",
        "\n",
        "        next_state = preprocess(real_data)\n",
        "        agent.update(state, action, reward, next_state)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQq45cIEnXb5",
        "outputId": "eda23e48-6b70-4872-8be2-82b5f454d7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Training complete\n",
            "Epoch [2/100] Training complete\n",
            "Epoch [3/100] Training complete\n",
            "Epoch [4/100] Training complete\n",
            "Epoch [5/100] Training complete\n",
            "Epoch [6/100] Training complete\n",
            "Epoch [7/100] Training complete\n",
            "Epoch [8/100] Training complete\n",
            "Epoch [9/100] Training complete\n",
            "Epoch [10/100] Training complete\n",
            "Epoch [11/100] Training complete\n",
            "Epoch [12/100] Training complete\n",
            "Epoch [13/100] Training complete\n",
            "Epoch [14/100] Training complete\n",
            "Epoch [15/100] Training complete\n",
            "Epoch [16/100] Training complete\n",
            "Epoch [17/100] Training complete\n",
            "Epoch [18/100] Training complete\n",
            "Epoch [19/100] Training complete\n",
            "Epoch [20/100] Training complete\n",
            "Epoch [21/100] Training complete\n",
            "Epoch [22/100] Training complete\n",
            "Epoch [23/100] Training complete\n",
            "Epoch [24/100] Training complete\n",
            "Epoch [25/100] Training complete\n",
            "Epoch [26/100] Training complete\n",
            "Epoch [27/100] Training complete\n",
            "Epoch [28/100] Training complete\n",
            "Epoch [29/100] Training complete\n",
            "Epoch [30/100] Training complete\n",
            "Epoch [31/100] Training complete\n",
            "Epoch [32/100] Training complete\n",
            "Epoch [33/100] Training complete\n",
            "Epoch [34/100] Training complete\n",
            "Epoch [35/100] Training complete\n",
            "Epoch [36/100] Training complete\n",
            "Epoch [37/100] Training complete\n",
            "Epoch [38/100] Training complete\n",
            "Epoch [39/100] Training complete\n",
            "Epoch [40/100] Training complete\n",
            "Epoch [41/100] Training complete\n",
            "Epoch [42/100] Training complete\n",
            "Epoch [43/100] Training complete\n",
            "Epoch [44/100] Training complete\n",
            "Epoch [45/100] Training complete\n",
            "Epoch [46/100] Training complete\n",
            "Epoch [47/100] Training complete\n",
            "Epoch [48/100] Training complete\n",
            "Epoch [49/100] Training complete\n",
            "Epoch [50/100] Training complete\n",
            "Epoch [51/100] Training complete\n",
            "Epoch [52/100] Training complete\n",
            "Epoch [53/100] Training complete\n",
            "Epoch [54/100] Training complete\n",
            "Epoch [55/100] Training complete\n",
            "Epoch [56/100] Training complete\n",
            "Epoch [57/100] Training complete\n",
            "Epoch [58/100] Training complete\n",
            "Epoch [59/100] Training complete\n",
            "Epoch [60/100] Training complete\n",
            "Epoch [61/100] Training complete\n",
            "Epoch [62/100] Training complete\n",
            "Epoch [63/100] Training complete\n",
            "Epoch [64/100] Training complete\n",
            "Epoch [65/100] Training complete\n",
            "Epoch [66/100] Training complete\n",
            "Epoch [67/100] Training complete\n",
            "Epoch [68/100] Training complete\n",
            "Epoch [69/100] Training complete\n",
            "Epoch [70/100] Training complete\n",
            "Epoch [71/100] Training complete\n",
            "Epoch [72/100] Training complete\n",
            "Epoch [73/100] Training complete\n",
            "Epoch [74/100] Training complete\n",
            "Epoch [75/100] Training complete\n",
            "Epoch [76/100] Training complete\n",
            "Epoch [77/100] Training complete\n",
            "Epoch [78/100] Training complete\n",
            "Epoch [79/100] Training complete\n",
            "Epoch [80/100] Training complete\n",
            "Epoch [81/100] Training complete\n",
            "Epoch [82/100] Training complete\n",
            "Epoch [83/100] Training complete\n",
            "Epoch [84/100] Training complete\n",
            "Epoch [85/100] Training complete\n",
            "Epoch [86/100] Training complete\n",
            "Epoch [87/100] Training complete\n",
            "Epoch [88/100] Training complete\n",
            "Epoch [89/100] Training complete\n",
            "Epoch [90/100] Training complete\n",
            "Epoch [91/100] Training complete\n",
            "Epoch [92/100] Training complete\n",
            "Epoch [93/100] Training complete\n",
            "Epoch [94/100] Training complete\n",
            "Epoch [95/100] Training complete\n",
            "Epoch [96/100] Training complete\n",
            "Epoch [97/100] Training complete\n",
            "Epoch [98/100] Training complete\n",
            "Epoch [99/100] Training complete\n",
            "Epoch [100/100] Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TESTING PHASE"
      ],
      "metadata": {
        "id": "Uzpkn6acuUju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = torch.randn(10, sensor_data_dim)\n",
        "\n",
        "actions = [agent.choose_action(state) for state in test_data]\n",
        "\n",
        "# Check for anomalies using the discriminator\n",
        "for i, data_point in enumerate(test_data):\n",
        "    is_anomalous = is_anomaly(data_point.unsqueeze(0))\n",
        "    action_taken = \"Anomaly\" if actions[i] == 1 else \"Normal\"\n",
        "    print(f\"Data point {i+1}: Predicted as {action_taken}, Discriminator says {'Anomaly' if is_anomalous else 'Normal'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9zGq5XrnZJI",
        "outputId": "7e925815-bc78-4ba2-c3d2-d5a11d886f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data point 1: Predicted as Normal, Discriminator says Anomaly\n",
            "Data point 2: Predicted as Normal, Discriminator says Anomaly\n",
            "Data point 3: Predicted as Normal, Discriminator says Anomaly\n",
            "Data point 4: Predicted as Normal, Discriminator says Anomaly\n",
            "Data point 5: Predicted as Normal, Discriminator says Anomaly\n",
            "Data point 6: Predicted as Normal, Discriminator says Anomaly\n",
            "Data point 7: Predicted as Normal, Discriminator says Normal\n",
            "Data point 8: Predicted as Normal, Discriminator says Normal\n",
            "Data point 9: Predicted as Normal, Discriminator says Anomaly\n",
            "Data point 10: Predicted as Normal, Discriminator says Anomaly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_GoIoAhqRFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}